{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-4. \n",
        "Take any text file and now your task is to Text Summarization without using hugging transformer library"
      ],
      "metadata": {
        "id": "1b_CeBBEB3Dj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OnT5NsP_Bmiu"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "\n",
        "import nltk   # nltk is for natural language processing and computational linguistics\n",
        "from nltk.corpus import stopwords   # corpus is a collection of authentic text or audio organized into datasets\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize    # sent_tokenize() method to split a document or paragraph into sentences, word_tokenize() method to split a sentence into tokens or words\n",
        "from nltk.probability import FreqDist   # FreqDist() gives the user the frequency distribution of all the words in the text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')     # used to eliminate unimportant words (commonly used words)\n",
        "nltk.download('punkt')    # a tokenizer that divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPymv1KaEU4S",
        "outputId": "d84b9236-7d5a-479a-e606-510f6e7e0119"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_summarization(text, num_sentences):\n",
        "    '''\n",
        "    This function takes text as input and number of sentenses required to as summary\n",
        "    and returns the summary of the text in definef number of lines\n",
        "    '''\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    \n",
        "    # Tokenize the sentences into words\n",
        "    words = [word_tokenize(sentence) for sentence in sentences]\n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [[word.lower() for word in sentence if word.isalnum() and word.lower() not in stop_words] for sentence in words]\n",
        "    \n",
        "    # Calculate word frequency distribution\n",
        "    word_frequencies = FreqDist([word for sentence in words for word in sentence])\n",
        "    \n",
        "    # Calculate sentence scores based on word frequencies\n",
        "    sentence_scores = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        score = sum([word_frequencies[word.lower()] for word in words[i]])\n",
        "        sentence_scores.append((i, sentence, score))\n",
        "    \n",
        "    # Sort the sentences based on scores in descending order\n",
        "    sentence_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    # Select the top sentences to form the summary\n",
        "    summary_sentences = [sentence for _, sentence, _ in sentence_scores[:num_sentences]]\n",
        "    \n",
        "    # Join the summary sentences to form the final summary\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    \n",
        "    return summary"
      ],
      "metadata": {
        "id": "Znf_0B5OCF-0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text from a file\n",
        "\n",
        "file_path = '/content/nlp_details.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "vBAkYV_7CMSn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of sentences in the summary\n",
        "\n",
        "num_sentences = 10"
      ],
      "metadata": {
        "id": "tNK17N7nCUSI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform text summarization\n",
        "\n",
        "summary = text_summarization(text, num_sentences)"
      ],
      "metadata": {
        "id": "_dNL0ixvEKWe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sATwvOAENOh",
        "outputId": "8203a542-be8f-480c-f38b-5396281f47ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three tools used commonly for natural language processing include Natural Language Toolkit (NLTK), Gensim and Intel natural language processing Architect. The functions listed above are used in a variety of real-world applications, including:\n",
            "\n",
            "customer feedback analysis -- where AI analyzes social media reviews;\n",
            "customer service automation -- where voice assistants on the other end of a customer service phone line are able to use speech recognition to understand what the customer is saying, so that it can direct the call correctly;\n",
            "automatic translation -- using tools such as Google Translate, Bing Translator and Translate Me;\n",
            "academic research and analysis -- where AI is able to analyze huge amounts of academic material and research papers not just based on the metadata of the text, but the text itself;\n",
            "analysis and categorization of medical records -- where AI uses insights to predict, and ideally prevent, disease;\n",
            "word processors used for plagiarism and proofreading -- using tools such as Grammarly and Microsoft Word;\n",
            "stock forecasting and insights into financial trading -- using AI to analyze market history and 10-K documents, which contain comprehensive summaries about a company's financial performance;\n",
            "talent recruitment in human resources; and\n",
            "automation of routine litigation tasks -- one example is the artificially intelligent attorney. Challenges of natural language processing\n",
            "There are a number of challenges of natural language processing and most of them boil down to the fact that natural language is ever-evolving and always somewhat ambiguous. Deep learning models require massive amounts of labeled data for the natural language processing algorithm to train on and identify relevant correlations, and assembling this kind of big data set is one of the main hurdles to natural language processing. Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. Techniques and methods of natural language processing\n",
            "Syntax and semantic analysis are two main techniques used with natural language processing. Whether the language is spoken or written, natural language processing uses artificial intelligence to take real-world input, process it, and make sense of it in a way a computer can understand. Using a combination of machine learning, deep learning and neural networks, natural language processing algorithms hone their own rules through repeated processing and learning. This involves using natural language processing algorithms to analyze unstructured data and automatically produce content based on that data. Earlier approaches to natural language processing involved a more rules-based approach, where simpler machine learning algorithms were told what words and phrases to look for in text and given specific responses when those phrases appeared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bA5kEVmsEe2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}